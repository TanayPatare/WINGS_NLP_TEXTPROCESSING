{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                            task 1                                                              \n",
    "Define a function called processRawText, which takes a parameter. The first parameter textURL is an URL link. The function definition code stub is given in the editor. Perform the following tasks:\n",
    "\n",
    "Read the text content from the given link textURL. Store the content in the variable textcontent.\n",
    "\n",
    "Tokenize all the words in the textcontent, and convert them into lower case. Store the tokenized list of words in tokenizedicwords. (Hint: Use word tokenize)\n",
    "\n",
    "Find the number of words in 'tokenizedicwords, and store the result in noofwords.\n",
    "\n",
    "Find the number of unique words in tokenizedicwords, and store the result in noofunqwords\".\n",
    "\n",
    "Calculate the word coverage of \"tokenizedicwords obtained from the number of words and number of unique words, Store the result in the wordcov\n",
    "\n",
    "Determine the frequency distribution of all words having only alphabets in tokenizedicwords\". Store the result in the variable 'wordfred.\n",
    "\n",
    "• Find the maximum frequent word of tokenizedicwords. Store the result in the variable 'maxfred.                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def processRawText(textURL):\n",
    "    response = requests.get(textURL)\n",
    "    textcontent = response.text\n",
    "    \n",
    "    tokenized = [word.lower() for word in word_tokenize(textcontent)]\n",
    "    \n",
    "    noofwords = len(tokenized)\n",
    "    noofunqwords = len(list(set(tokenized)))\n",
    "    wordcov = int(noofwords/noofunqwords)\n",
    "    wordfreq = Counter(word for word in tokenized if word.isalpha())\n",
    "    maxfreq = wordfreq.most_common(1)[0][0]\n",
    "    \n",
    "    return noofwords, noofunqwords, wordcov, maxfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                        task2                                                                  \n",
    "Define a function called performBigramsAndCollocations\", which takes two parameters. The first parameter, textcontent', is a string, and the second parameter is 'word'. The function definition code stub is given in the editor. Perform the following tasks:\n",
    "\n",
    "Tokenize all the words given in textcontent. The word should contain alphabets or numbers or underscore. Store the tokenized list of words in tokenizedwords\". (Hint: Use regexp_tokenize)\n",
    "\n",
    "Convert all the words into lowercase. Store the result in tokenizedwords\".\n",
    "\n",
    "Compute bigrams of the list tokenizedwords. Store the list of bigrams in tokenizedwordsbigrams.\n",
    "\n",
    "Filter only the bigrams from tokenizedwordsbigrams, where the words are not part of stopwords. Store the result in tokenizednonstopwordsbigrams (Hint: Use stopwords corpora)\n",
    "\n",
    "• Compute the conditional frequency of tokenizednonstopwordsbigrams where condition and event refer to the words. Store the result in 'cfd bigrams.\n",
    "\n",
    "Determine the three most frequent words occurring after the given word. Store the result in mostfrequentwordafter'.\n",
    "\n",
    "Generate collocations from tokenizedwords. Store list of collocation words in collocationwords'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.util import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "\n",
    "def performBigramsAndCollocations(textcontent, word):\n",
    "    \n",
    "    tokenizedwords = [word.lower() for word in regexp_tokenize(textcontent, r'\\w+')]\n",
    "    tokenizedwordsbigrams = list(bigrams(tokenizedwords))\n",
    "    stop_word = list(stopwords.words('english'))\n",
    "    tokenizednonstopwordsbigrams = [bigram for bigram in tokenizedwordsbigrams if    bigram[0] not in stop_word and bigram[1] not in stop_word]\n",
    "    \n",
    "    cfd_bigrams = ConditionalFreqDist(tokenizednonstopwordsbigrams)\n",
    "    mostfrequentwordafter = cfd_bigrams[word].most_common(3)\n",
    "    \n",
    "    text = nltk.Text(tokenizedwords)\n",
    "    collocationwords1 = text.collocation_list()\n",
    "    collocationwords = []\n",
    "    for i in collocationwords1:\n",
    "        L = []\n",
    "        for j in i:\n",
    "            L.append(j)\n",
    "        string = \" \".join(L)\n",
    "        collocationwords.append(string)\n",
    "    \n",
    "    return mostfrequentwordafter,collocationwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                        task3                                                                 \n",
    "Define a function called performStemAndLemma\", which takes a parameter. The first parameter, textcontent, is a string. The function definition code stub is given in the editor. Perform the following specified tasks:\n",
    "\n",
    "Tokenize all the words given in textcontent. The word should contain alphabets or numbers or underscore. Store the tokenized list of words in tokenizedwords\". (Hint: Use regexp tokenize)\n",
    "\n",
    "Convert all the words into lowercase from the unique set of tokenizedwords. Store the result into the variable tokenizedwords.\n",
    "\n",
    "Renove all the stop words from the tokenizedwords. Store the result into the variabl filteredwords. (Hint: Use stopwords corpora)\n",
    "\n",
    "Stem each word present in filteredwords with PorterStemmer, and store the result in the list porterstemmedwords\".\n",
    "\n",
    "Stem each word present in filteredwords with LancasterStemmer, and store the result in the list lancasterstemmedwords.\n",
    "\n",
    "Lemmatize each word present in filteredwords with WordNetLemmatizer, and store the result in the list lemmatizedwords.          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "def performStemAndLemma(textcontent):\n",
    "    tokenizedwords1 = list(set(regexp_tokenize(textcontent,r'\\w+')))\n",
    "    tokenizedwords = [w.lower() for w in tokenizedwords1]\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    filteredwords = [w for w in tokenizedwords if w not in stop_words]\n",
    "    \n",
    "    port = PorterStemmer()\n",
    "    lan = LancasterStemmer()\n",
    "    word = WordNetLemmatizer()\n",
    "    \n",
    "    porterstemmedwords = [port.stem(filteredword) for filteredword in filteredwords]\n",
    "    lancasterstemmedwords = [lan.stem(filteredword) for filteredword in filteredwords]\n",
    "    lemmatizedwords = [word.lemmatize(filteredword) for filteredword in filteredwords]\n",
    "    \n",
    "    return porterstemmedwords, lancasterstemmedwords, lemmatizedwords\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                            task4                                                             \n",
    "The first parameter, textcontent', is a string, the second parameter, taggedtextcontent', is also a string, and the third parameter, 'defined_tags', is a dictionary. The function definition code stub is given in the editor. Perform the following specified tasks:\n",
    "\n",
    "ALL\n",
    "\n",
    "Tag the Part of Speech for the given textcontent words, store the result into the variable nitk_pos_tags. (Hint: Use pos_tag)\n",
    "\n",
    "Tag the Part of Speech for the given taggedtextcontent words using the Tagging Text method. Store the result into the variable tagged_pos_tag.\n",
    "\n",
    "Tag the Part of Speech for the given textcontent words and use defined tags as a model in the Lookup Tagger method. Store the result into the variable unigram_pos_tag.\n",
    "\n",
    "Return nitk_pos_tags', tagged_pos_tag, unigram_pos_tag variables from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tagPOS(textcontent, taggedtextcontent, defined_tags):\n",
    "    token = word_tokenize(textcontent)\n",
    "    nltk_pos_tags = pos_tag(token)\n",
    "    \n",
    "    L = taggedtextcontent.split()\n",
    "    tagged_pos_tag = []\n",
    "    for i in L:\n",
    "        tagged_pos_tag.append(tuple(i.split('/')))\n",
    "        \n",
    "    unigram = nltk.UnigramTagger(model=defined_tags)\n",
    "    unigram_pos_tag = unigram.tag(token)\n",
    "    \n",
    "    return nltk_pos_tags, tagged_pos_tag, unigram_pos_tag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
